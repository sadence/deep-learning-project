[DEFAULT]
seq_length = 200
batch_size = 100
input_size = 80
hidden_size = 256
num_layers = 2
num_epochs = 1
learning_rate = 0.00146
dropout = 0

[WORD-LSTM]
seq_length = 200
batch_size = 100
input_size = 80
hidden_size = 100
num_layers = 2
num_epochs = 1
learning_rate = 0.01
dropout = 0
